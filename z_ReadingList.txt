<Basic/Classic RNNs>
-Maheswaranathan, N., & Sussillo, D. (2020). How recurrent networks implement contextual processing in sentiment analysis. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2004.08013
-Rigotti, M., Ben Dayan Rubin, D., Morrison, S. E., Salzman, C. D., & Fusi, S. (2010). Attractor concretion as a mechanism for the formation of context representations. NeuroImage, 52(3), 833–847.


<Psych/Neuro>
-Flesch, T., Nagy, D. G., Saxe, A., & Summerfield, C. (2022). Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals. In arXiv [q-bio.NC]. arXiv. http://arxiv.org/abs/2203.11560
-Bouchacourt, F., Palminteri, S., Koechlin, E., & Ostojic, S. (2020). Temporal chunking as a mechanism for unsupervised learning of task-sets. eLife, 9. https://doi.org/10.7554/eLife.50469
-Kim, J. Z., Soffer, J. M., Kahn, A. E., Vettel, J. M., Pasqualetti, F., & Bassett, D. S. (2018). Role of Graph Architecture in Controlling Dynamical Networks with Applications to Neural Systems. Nature Physics, 14, 91–98.
-Lynn, C. W., & Bassett, D. S. (2019). The physics of brain network structure, function and control. Nature Reviews Physics, 1(5), 318–332.
-https://www.youtube.com/watch?v=bCQQ7icqPWg
-Kamiya, S., Kawakita, G., Sasai, S., Kitazono, J., & Oizumi, M. (2022). Optimal Control Costs of Brain State Transitions in Linear Stochastic Systems. In bioRxiv (p. 2022.05.01.490252). https://doi.org/10.1101/2022.05.01.490252
-Mante, V., Sussillo, D., Shenoy, K. V., & Newsome, W. T. (2013). Context-dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474), 78–84.
-Egger, S. W., Remington, E. D., Chang, C.-J., & Jazayeri, M. (2019). Internal models of sensorimotor integration regulate cortical dynamics. Nature Neuroscience. https://doi.org/10.1038/s41593-019-0500-6
-Sussillo, D., Churchland, M. M., Kaufman, M. T., & Shenoy, K. V. (2015). A neural network that finds a naturalistic solution for the production of muscle activity. Nature Neuroscience, 18(7), 1025–1033.
-Disentangling with Biological Constraints: A Theory of Functional Cell Types


<RNN contrains>
-low-rank RNN: Mastrogiuseppe, F., & Ostojic, S. (2018). Linking connectivity, dynamics, and computations in low-rank recurrent neural networks. Neuron, 99(3), 609–623.e29.
-Bernáez Timón, L., Ekelmans, P., Kraynyukova, N., Rose, T., Busse, L., & Tchumatchenko, T. (2022). How to incorporate biological insights into network models and why it matters. The Journal of Physiology. https://doi.org/10.1113/JP282755
-Sussillo, D., Jozefowicz, R., Abbott, L. F., & Pandarinath, C. (2016). LFADS - Latent Factor Analysis via Dynamical Systems. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1608.06315


<Representations>
-https://transformer-circuits.pub/2022/toy_model/index.html: Superposition
-Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., & Lerchner, A. (2022). beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. https://openreview.net/pdf?id=Sy2fzU9gl
-Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., & Lerchner, A. (2018). Understanding disentangling in β-VAE. In arXiv [stat.ML]. arXiv. http://arxiv.org/abs/1804.03599



