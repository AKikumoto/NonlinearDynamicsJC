{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n",
      "DIR_HERE\t D_sinewave_pattern\t F\t L_default_weights\t Trainer\t Variable\t datasets\t device\t nn\t \n",
      "np\t optim\t os\t plt\t pp\t summary\t sys\t torch\t transforms\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nonlinear dynamics JC\n",
    "# save wave pattern generation task example\n",
    "# (note linear approximation part is not implemented yet!)\n",
    "# AK, July-1-2022\n",
    "# =========================================================================\n",
    "%reset -f\n",
    "import os, sys\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Define paths \n",
    "DIR_HERE = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "sys.path.append(\"../../PyTorch_library/\")\n",
    "\n",
    "# Load modules \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch related functions \n",
    "from PyTorch_data import D_sinewave_pattern\n",
    "from PyTorch_utils import L_default_weights, Trainer \n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used:\", device)\n",
    "\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mD_sinewave_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m612\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Sine wave pattern generation (continuous)\n",
       "Using constant frequency value, generate sine waves\n",
       "n_sample = max number of batch\n",
       "n_time = how many time samples to make\n",
       "n_freq = range of frequency of sine wave\n",
       "(x) = generated dataset\n",
       "(t) = continuous sine wave\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Dropbox/w_LABWORKS/STUDYGROUPS/DeepLearningFromScratch/PyTorch_library/PyTorch_data.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?D_sinewave_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "# Simulate data\n",
    "n_batch = 10\n",
    "n_time = 40\n",
    "n_freq = 10\n",
    "data = D_sinewave_pattern(n_time = n_time, n_freq = n_freq, max_sample = n_batch)\n",
    "\n",
    "# Data loader for training\n",
    "train_dataloader = torch.utils.data.DataLoader(data,\n",
    "                                            batch_size = n_batch,\n",
    "                                            shuffle = True)\n",
    " \n",
    "# Checking loader\n",
    "for i, (inputs, labels) in (enumerate(train_dataloader)):print(\"done\")\n",
    "# plt.plot(labels[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn): RNN(1, 200, batch_first=True)\n",
      "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Net                                      --\n",
      "├─RNN: 1-1                               40,600\n",
      "│    └─weight_ih_l0                      ├─200\n",
      "│    └─weight_hh_l0                      ├─40,000\n",
      "│    └─bias_ih_l0                        ├─200\n",
      "│    └─bias_hh_l0                        └─200\n",
      "├─Linear: 1-2                            201\n",
      "│    └─weight                            ├─200\n",
      "│    └─bias                              └─1\n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------\n",
    "# Define hyperparameters\n",
    "n_epochs = 2000         # how many times to repeat learning epochs\n",
    "learning_rate = .001    # learning rate of the oprimizer\n",
    "\n",
    "# Layer properties\n",
    "n_input = 1             # constant frequency value at time n\n",
    "n_output = 1            # sine wave value at time n\n",
    "n_rnn_layers = 1        # number of rnn layers stacked\n",
    "n_units_hidden = 200    # number of hidden units\n",
    "rnn_nl = 'relu'         # nonlinear update function of rnn layer\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# Define neural network model (dense feedforward network)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_time, n_input, n_output, n_units_hidden, n_rnn_layers, rnn_nl):\n",
    "        # Initialize module\n",
    "        super(Net, self).__init__()\n",
    "        self.n_time = n_time\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_units_hidden = n_units_hidden\n",
    "        self.n_rnn_layers = n_rnn_layers\n",
    "        \n",
    "        # Prepare layers\n",
    "        self.rnn = nn.RNN(n_input, n_units_hidden, nonlinearity = rnn_nl, dropout = 0.0, batch_first = True)\n",
    "        self.fc = nn.Linear(n_units_hidden, n_output)\n",
    "        \n",
    "        # Initialize weights (to use defalt comment out all)\n",
    "        #self.apply(self._init_weights)\n",
    "        #self.apply(L_default_weights) # from PyTorch_util\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        print(\"To modify initial weights modify this part\")\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _set_state(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.n_rnn_layers, self.n_batch, self.n_units_hidden).zero_())\n",
    "    \n",
    "    # def forward\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        # Initialize hidden state\n",
    "        self.n_batch = x.shape[0] # batch_first = True\n",
    "        self.h_state = self._set_state()\n",
    "        \n",
    "        # Transform data: (Batch, Time, Input)\n",
    "        x = x.view(self.n_batch, self.n_time, self.n_input)\n",
    "        output = torch.zeros(self.n_batch, self.n_time, self.n_output).type_as(x.data)\n",
    "        \n",
    "        # Connect layers and pass on to activation functions\n",
    "        #(rnn_out) = (batch, time, hidden_size)\n",
    "        #(h_n) =  (num_layers, batch, hidden_size)\n",
    "        h_all, h_n = self.rnn(x, self.h_state) # loop over n_time inside of rnn module\n",
    "        \n",
    "        # Hidden state at each time point is passed on to same output layer\n",
    "        # https://github.com/tripdancer0916/pytorch-fixed-point-analysis/blob/master/model.py\n",
    "        for t in range(self.n_time):output[:,t,:] = self.fc(h_all[:, t, :])\n",
    "\n",
    "        return output, h_all , h_n\n",
    "\n",
    "#------------------------------\n",
    "# Instantiate model\n",
    "model = Net(n_time, n_input, n_output, n_units_hidden, n_rnn_layers, rnn_nl).to(device)\n",
    "print(model)\n",
    "summary(model, verbose = 2)\n",
    "\n",
    "#------------------------------\n",
    "# Define loss function\n",
    "# CrossEntropyLoss = softmax + cross entropy error\n",
    "criterion =  torch.nn.MSELoss()\n",
    "\n",
    "#------------------------------\n",
    "# Define optimier\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000, Loss: 0.4736787676811218, Acc: 24.488804626464844\n",
      "Epoch: 201/2000, Loss: 0.3917607069015503, Acc: 20.577940368652342\n",
      "Epoch: 401/2000, Loss: 0.03259832039475441, Acc: 5.083506011962891\n",
      "Epoch: 601/2000, Loss: 0.0035950392484664917, Acc: 1.9182491302490234\n",
      "Epoch: 801/2000, Loss: 0.0015359095996245742, Acc: 1.266917610168457\n",
      "Epoch: 1001/2000, Loss: 0.00032629817724227905, Acc: 0.5556034564971923\n",
      "Epoch: 1201/2000, Loss: 0.001100083696655929, Acc: 1.0845407485961913\n",
      "Epoch: 1401/2000, Loss: 0.0022626304998993874, Acc: 1.2373466491699219\n",
      "Epoch: 1601/2000, Loss: 0.001049889251589775, Acc: 0.8808669090270996\n",
      "Epoch: 1801/2000, Loss: 0.0003075606655329466, Acc: 0.5693301677703857\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "# Do training (make sure to get new model, criterion, optimizer)\n",
    "model.train()  # change model as \"training mode\"\n",
    "\n",
    "# Loop over n_epoch times\n",
    "for epoch in range(n_epochs): \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:        \n",
    "        # CPU/GPU stuff (don't worry)\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).float()\n",
    "        \n",
    "        # Initialize gradient of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward path\n",
    "        outputs, h_all, h_n = model(inputs)\n",
    "        \n",
    "        # Calculate loss: difference between output and label\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward path (Backpropagation!)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keep track of the progress of learning\n",
    "        loss_sum += loss\n",
    "        acc_sum += torch.sum(torch.abs(targets - outputs))\n",
    "\n",
    "    # Display learning progress\n",
    "    if  epoch % 200 == 0:\n",
    "        epoch_loss = loss_sum.item() / len(train_dataloader) # cumulative loss/ batch size\n",
    "        epoch_acc = acc_sum.double() / len(train_dataloader.dataset) # cummlative count of correct prediction / data size\n",
    "        print(f\"Epoch: {epoch+1}/{n_epochs}, Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "    # モデルの重みの保存\n",
    "    #torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
