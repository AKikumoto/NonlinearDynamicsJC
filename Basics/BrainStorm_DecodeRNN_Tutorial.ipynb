{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR_HERE\t F\t L_default_weights\t Trainer\t Variable\t datasets\t nn\t np\t optim\t \n",
      "os\t plt\t pp\t summary\t sys\t torch\t transforms\t \n"
     ]
    }
   ],
   "source": [
    "# BrainStrom 2022\n",
    "# Tutorial for classification with Neural nework (recurrent network)\n",
    "# Resources:\n",
    "# - https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "# AK, June-26-2022\n",
    "# =========================================================================\n",
    "%reset -f\n",
    "import os, sys\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Define paths \n",
    "DIR_HERE = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "sys.path.append('../Others')\n",
    "\n",
    "# Load basic modules \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step0: Before we start... \n",
    "# -----------------------------\n",
    "\n",
    "# What is MNIST dataset?\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step1: Prepare data\n",
    "# -----------------------------\n",
    "\n",
    "# Define \"transform\", which adjusts data \n",
    "# https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),                      # [H, W, C] -> [C, H, W]\n",
    "     transforms.Normalize((0.5, ), (0.5, ))])    # normalize (mean =.5, sd = .5)\n",
    "\n",
    "\n",
    "# Load (download for the first run) MNIST data\n",
    "# Each image (C = channel, H = height, W = width) = (1, 28, 28) = 784 pixels\n",
    "# https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST\n",
    "\n",
    "#Train data (channel, height, width) = (1,28,28) 60000 images\n",
    "train_dataset = datasets.MNIST(root='./data', \n",
    "                                        train = True, # Get training dataset\n",
    "                                        download = True,\n",
    "                                        transform = transform)\n",
    "\n",
    "#Test data (channel, height, width) = (1,28,28) 10000 images\n",
    "test_dataset = datasets.MNIST(root='./data', \n",
    "                                        train = False, # Get test dataset\n",
    "                                        download = True, \n",
    "                                        transform = transform)\n",
    "\n",
    "# Data too large?\n",
    "# RandomSubsetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size =  torch.Size([1, 28, 28])\n",
      "labels =  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label = 5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking data and iteractor behavior\n",
    "batch_iterator = iter(train_dataset) \n",
    "image, label = next(batch_iterator) \n",
    "print(\"image size = \", image.size())\n",
    "print(\"labels = \", label)\n",
    "\n",
    "plt.imshow(image[0].numpy().reshape(28,28), cmap='gray')\n",
    "plt.title(\"Label = %d\" % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Instantiate model\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m model \u001b[38;5;241m=\u001b[39m Net(n_time, n_input, n_output, n_units_hidden, n_rnn_layers, rnn_nl)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     67\u001b[0m summary(model, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 2: Define model\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# Define hyperparameters\n",
    "n_epochs = 3            # how many times to repeat learning epochs\n",
    "n_batch = 60           # how many examples to give in one epoch\n",
    "learning_rate = .001    # learning rate of the oprimizer\n",
    "\n",
    "# Layer properties\n",
    "n_time = 28             # use heigh of image as sequential input\n",
    "n_input = 28            # use width of image as features (input)\n",
    "n_output = 10           # number of target (i.e., unique hand-written numbers)\n",
    "n_rnn_layers = 1        # number of rnn layers stacked\n",
    "n_units_hidden = 128    # number of hidden units\n",
    "rnn_nl = 'relu'         # nonlinear update function of rnn layer\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# Define neural network model (dense feedforward network)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_time, n_input, n_output, n_units_hidden, n_rnn_layers, rnn_nl):\n",
    "        # Initialize module\n",
    "        super(Net, self).__init__()\n",
    "        self.n_time = n_time\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_units_hidden = n_units_hidden\n",
    "        self.n_rnn_layers = n_rnn_layers\n",
    "        \n",
    "        # Prepare layers\n",
    "        self.rnn = nn.RNN(n_input, n_units_hidden, nonlinearity = rnn_nl, dropout = 0.0, batch_first = True)\n",
    "        self.fc = nn.Linear(n_units_hidden, n_output)\n",
    "        \n",
    "        # Initialize weights (to use defalt comment out all)\n",
    "        #self.apply(self._init_weights)\n",
    "        #self.apply(L_default_weights) # from PyTorch_util\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        print(\"To modify initial weights modify this part\")\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _set_state(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.n_rnn_layers, self.n_batch, self.n_units_hidden).zero_())\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        # Initialize hidden state\n",
    "        self.n_batch = x.shape[0] # batch_first = True\n",
    "        self.h_state = self._set_state()\n",
    "        \n",
    "        # Transform data: (Batch, Time, Input)\n",
    "        x = x.view(self.n_batch, self.n_time, self.n_input)\n",
    "        \n",
    "        # Connect layers and pass on to activation functions\n",
    "        #(rnn_out) = (batch, time, hidden_size)\n",
    "        #(h_n) =  (num_layers, batch, hidden_size)\n",
    "        rnn_out, h_n = self.rnn(x, self.h_state) \n",
    "        x = h_n[-1, :, :]  # get last state of the hidden layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "# Instantiate model\n",
    "model = Net(n_time, n_input, n_output, n_units_hidden, n_rnn_layers, rnn_nl).to(device)\n",
    "print(model)\n",
    "summary(model, verbose = 2)\n",
    "# summary(model, (n_batch, n_input, n_units_hidden), verbose = 2, col_width=16, col_names=[\"kernel_size\", \"output_size\", \"num_params\"])\n",
    "\n",
    "#------------------------------\n",
    "# Define loss function\n",
    "# CrossEntropyLoss = softmax + cross entropy error\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#------------------------------\n",
    "# Define optimier\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#------------------------------\n",
    "# Define data loader, which generates batch efficiently in PyTorch\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# https://blog.paperspace.com/dataloaders-abstractions-pytorch/\n",
    "\n",
    "# Data loader for training\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size = n_batch,\n",
    "                                            shuffle = True)\n",
    "\n",
    "#Data loader for testing\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                            batch_size = n_batch,\n",
    "                                            shuffle = False)\n",
    "\n",
    "# vars(model.rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 0.8071481323242188, Acc: 0.7265\n",
      "Epoch: 2/5, Loss: 0.32360336303710935, Acc: 0.9019166666666667\n",
      "Epoch: 3/5, Loss: 0.22955712890625, Acc: 0.93175\n",
      "Epoch: 4/5, Loss: 0.19020013427734375, Acc: 0.9434333333333333\n",
      "Epoch: 5/5, Loss: 0.1607078857421875, Acc: 0.9526\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 3: Train model\n",
    "# -----------------------------\n",
    "\n",
    "#------------------------------\n",
    "# change model as \"training mode\"\n",
    "model.train()  \n",
    "\n",
    "# Loop over n_epoch times\n",
    "for epoch in range(n_epochs): \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader:        \n",
    "        # CPU/GPU stuff (don't worry)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Initialize gradient of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward path\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss: difference between output and label\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Keep track of the progress of learning\n",
    "        loss_sum += loss\n",
    "        _, preds = torch.max(outputs, 1) # prediction = max softmax output\n",
    "        acc_sum += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Backward path (Backpropagation!)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Display learning progress\n",
    "    epoch_loss = loss_sum.item() / len(train_dataloader) # cumulative loss/ batch size\n",
    "    epoch_acc = acc_sum.double() / len(train_dataloader.dataset) # cummlative count of correct prediction / data size\n",
    "    print(f\"Epoch: {epoch+1}/{n_epochs}, Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "    # Save weights\n",
    "    #torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate model\n",
    "# -----------------------------\n",
    "\n",
    "# Prepare test data to handle easily \n",
    "batch_iterator = iter(test_dataloader) # Make iterator out ot test_dataloader\n",
    "images, labels = next(batch_iterator)  # Get first group of test batch\n",
    "model.eval()  # change model as \"evaluating mode\"\n",
    "\n",
    "# Make predictions to one test batch\n",
    "with torch.set_grad_enabled(False):    # stop calculating gradient (stop learning)\n",
    "    outputs = model(images)            # forward path\n",
    "    _, preds = torch.max(outputs, 1)   # make prediction (take max of softmax function)\n",
    "\n",
    "    \n",
    "# Plot predicted label and correct label\n",
    "_, axs = plt.subplots(5, 2, figsize=(20, 20))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i in range(len(preds)): \n",
    "    axs[i].imshow(images[i].numpy().reshape(28,28), cmap='gray')\n",
    "    axs[i].title.set_text(\"Label: Target={}, Predict={}\".format(labels[i].numpy(), preds[i].numpy()))\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
